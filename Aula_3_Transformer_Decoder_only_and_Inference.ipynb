{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasleao03/GSI073---Topicos-Especiais-de-Inteligencia-Artificial-LLMs-Large-Language-Models-/blob/main/Aula_3_Transformer_Decoder_only_and_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 3 - Transformer *decoder-only*\n",
        "\n",
        "Nesta aula você irá modificar o Transformer *decoder-only* fornecido a seguir.\n",
        "Observe que em um *decoder-only* não existe:\n",
        "\n",
        "*   cross-attention\n",
        "*   encoder separado\n",
        "\n",
        "e é utilizado com *auto-regressão*.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "\n",
        "## Exercício\n",
        "\n",
        "Neste exercício você deve:\n",
        "\n",
        "1.   carregar o seu conjunto de documentos\n",
        "2.   treinar e usar (ou carregar) um tokenizador\n",
        "3.   fazer treino de um modelo decoder-only\n",
        "4.   incluir no loop de treino, inferência usando máxima probabilidade\n",
        "5.   incluir no loop de treino, inferência usando amostragem com temperatura\n"
      ],
      "metadata": {
        "id": "5WKD2sY0AuFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "aU6lT4FQAtGg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 1: carregar conjunto de documentos"
      ],
      "metadata": {
        "id": "ar5yEq-3H8oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "###### INSIRA AQUI O CODIGO PARA CARREGAR OS SEUS DOCUMENTOS na lista DOCUMENTOS\n",
        "#ds = load_dataset(\"jquigl/imdb-genres\") # Exemplo\n",
        "\n",
        "def clean_ascii(text):\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    return re.sub(r\"[^A-Za-z0-9 .,:;!?'\\-]\", \"\", text)\n",
        "\n",
        "# Extract documentos\n",
        "documentos = [clean_ascii(x[\"description\"]) for x in ds[\"train\"]]\n",
        "documentos = [t.split(\" - \")[0] for t in documentos]   # optional remove year\n",
        "documentos = [t for t in documentos if len(t) > 0]\n",
        "\n",
        "#documentos = [ \"meu doc favorito 1\", \"meu doc menos favorito 2\"]\n",
        "print(\"Total documentos:\", len(documentos))\n",
        "print(\"Sample:\", documentos[:10])"
      ],
      "metadata": {
        "id": "UNTLylj6c3HH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "collapsed": true,
        "outputId": "693e0389-496d-42f8-a010-ddf4b02db189"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1404575137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Extract documentos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdocumentos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdocumentos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" - \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocumentos\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# optional remove year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdocumentos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocumentos\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 2: Carregar ou treinar um tokenizador"
      ],
      "metadata": {
        "id": "SuLZ15ODIFeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "\n",
        "# -------- Ler dataset --------\n",
        "documentos = []\n",
        "\n",
        "with open(\"/content/pt-br.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for linha in f:\n",
        "        item = json.loads(linha)\n",
        "        documentos.append(item[\"fr\"])\n",
        "        documentos.append(item[\"pt\"])\n",
        "\n",
        "##### Insira aqui o código para treinar o seu TOKENIZER\n",
        "# Defina o seu tokenizador\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = WordLevelTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        ")\n",
        "# Treino do tokenizador\n",
        "tokenizer.train_from_iterator(documentos, trainer)\n",
        "\n",
        "# Mostrar alguns tokens do vocabulário\n",
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "for i, (token, id_) in enumerate(vocab.items()):\n",
        "    if i < 30:\n",
        "        print(token, id_)\n",
        "\n",
        "# Funções auxiliares para transitar entre tokens textuais e ids de tokens\n",
        "def encode(text):\n",
        "    ids = tokenizer.encode(\"[BOS] \" + text + \" [EOS]\").ids\n",
        "    return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "def decode(ids):\n",
        "    return tokenizer.decode(ids.tolist())\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n"
      ],
      "metadata": {
        "id": "7D2487loFG_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cbf71e-b480-4ff4-a927-9959baf54970"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "descansava 487\n",
            "por 909\n",
            "façades 850\n",
            "cuit 571\n",
            "odeur 82\n",
            "arabescos 975\n",
            "macia 758\n",
            "luz 54\n",
            "les 12\n",
            "sobre 52\n",
            "frescura 94\n",
            "lune 321\n",
            "cor 417\n",
            "savon 381\n",
            "grincer 864\n",
            "cannelle 514\n",
            "haute 741\n",
            "été 481\n",
            "blés 456\n",
            "insecte 460\n",
            "algodão 512\n",
            "bochechas 651\n",
            "cintilar 815\n",
            "délicieux 240\n",
            "vanille 429\n",
            "moído 527\n",
            "fondant 367\n",
            "momento 373\n",
            "pacificamente 405\n",
            "graveto 439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do modelo Transformer Decoder-only"
      ],
      "metadata": {
        "id": "EUXZOhS2Ima9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    \"Implementação de um transformer que tem somente a parte do decoder\"\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=3, max_len=64):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers=num_layers)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "\n",
        "        h = self.token_emb(x) + self.pos_emb(pos)\n",
        "\n",
        "        # Máscara causal\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "\n",
        "        out = self.decoder(h, h, tgt_mask=mask)\n",
        "        logits = self.lm_head(out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sZaswUVoDICQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Códigos de inferência"
      ],
      "metadata": {
        "id": "yxG3UcNjIub6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código de inferência simples: token com maior probabilidade\n"
      ],
      "metadata": {
        "id": "0o_3Z8ZWF1fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_prob_sampling(logits):\n",
        "    next_token = logits.argmax(dim=-1)\n",
        "    return next_token.unsqueeze(0)"
      ],
      "metadata": {
        "id": "DQP1EmACGBsF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código de inferência avançada: amostragem com temperatura"
      ],
      "metadata": {
        "id": "lqgdlkqHF6Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Inferência (com temperature e top_p)\n",
        "def sampling(logits, top_p=0.9, top_k=None, temperature=1.0):\n",
        "    # Ajusta pela temperatura\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Se top_k for especificado, filtra por top_k primeiro\n",
        "    if top_k is not None:\n",
        "        # Ensure top_k is not larger than the vocabulary size\n",
        "        k_to_use = min(top_k, logits.size(-1))\n",
        "        # Get the top_k values and indices\n",
        "        v, _ = torch.topk(logits, k_to_use)\n",
        "        # Set logits of all values smaller than the k-th value to -inf\n",
        "        logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "    # Ordena os logits\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "\n",
        "    # Mascara tokens acima do top_p\n",
        "    mask = cumulative_probs > top_p\n",
        "    # Garante que ao menos um token permaneça\n",
        "    mask[..., 1:] = mask[..., :-1].clone()\n",
        "    mask[..., 0] = False\n",
        "\n",
        "    filtered_logits = sorted_logits.masked_fill(mask, float('-inf'))\n",
        "    probs = torch.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "    # Amostra o token\n",
        "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    # Converte para índice na tabela original\n",
        "    next_token = sorted_indices[sampled_idx]\n",
        "\n",
        "    return next_token\n",
        "\n",
        "\n",
        "def generate(prompt, next_token_function, max_new_tokens=20, top_k=None, top_p=0.9, temperature=1.0):\n",
        "    model.eval()\n",
        "    x = encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(x)[:, -1, :]  # pega apenas o último passo\n",
        "\n",
        "        # Passa os parâmetros de amostragem se a função for top_p_sampling\n",
        "        if next_token_function == sampling:\n",
        "            next_token = next_token_function(logits.squeeze(0), top_k=top_k, top_p=top_p, temperature=temperature)\n",
        "        else:\n",
        "            next_token = next_token_function(logits.squeeze(0))\n",
        "\n",
        "        x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    return decode(x[0])\n"
      ],
      "metadata": {
        "id": "pGvot4bvd3il"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Fazer treino de modelo: códigos de treino"
      ],
      "metadata": {
        "id": "yFyl_KB-HAak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função auxiliar para gerar batches de exemplos para treino\n",
        "def sample_batch(batch_size=16, max_len=20):\n",
        "    batch = random.sample(documentos, batch_size)\n",
        "    tokenized = [encode(t) for t in batch]\n",
        "\n",
        "    max_t = min(max(len(x) for x in tokenized), max_len)\n",
        "    padded = []\n",
        "\n",
        "    for x in tokenized:\n",
        "        x = x[:max_t]\n",
        "        pad_len = max_t - len(x)\n",
        "        if pad_len > 0:\n",
        "            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])\n",
        "        padded.append(x)\n",
        "\n",
        "    return torch.stack(padded)\n",
        "\n",
        "################################\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DecoderOnlyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "#################################\n",
        "\n",
        "steps = 10000\n",
        "\n",
        "for step in range(1, steps + 1):\n",
        "    model.train()\n",
        "    batch = sample_batch().to(device)\n",
        "\n",
        "    logits = model(batch[:, :-1])\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(-1, vocab_size),\n",
        "        batch[:, 1:].reshape(-1),\n",
        "        ignore_index=tokenizer.token_to_id(\"[PAD]\")\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        ppl = torch.exp(loss).item()\n",
        "        print(f\"[step {step}] loss={loss.item():.4f}, ppl={ppl:.2f}\")\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(\"Generated text:\")\n",
        "        print(generate(\"J’ai senti le parfum du pain chaud dans la boulangerie. \", next_token_function=sampling ))\n",
        "        print(\"--------------------------------------\")\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "id": "_fMNjKHeds_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1ceb964-74e3-45ce-9c81-1b1e909439bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[step 50] loss=6.8443, ppl=938.52\n",
            "[step 100] loss=6.6833, ppl=798.96\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . roses gramado canto O floral dans voarem cachorro boulangerie fruit terre printemps tasse confortável former perfume cair crème noisette sombras\n",
            "--------------------------------------\n",
            "[step 150] loss=6.4080, ppl=606.69\n",
            "[step 200] loss=6.2673, ppl=527.06\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . tapis silêncio dentro citron perfeito cheveux laranja enfants Les antes vindo visages corria levemente seu água pele voler forêt ensolarado\n",
            "--------------------------------------\n",
            "[step 250] loss=6.1436, ppl=465.71\n",
            "[step 300] loss=6.0165, ppl=410.14\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . , le brincavam fauteuil árvores torta dos pássaros lever aux dormia campos frescura telhados frutas pele beira promener moelleux tombaient\n",
            "--------------------------------------\n",
            "[step 350] loss=5.8090, ppl=333.27\n",
            "[step 400] loss=5.9000, ppl=365.04\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . voler folha cachorro colorés paisagem fondantes aux paisible légère mer rua fraîcheur das épaules bouger lumineux cheia soprava quebrarem douceur\n",
            "--------------------------------------\n",
            "[step 450] loss=5.7268, ppl=306.98\n",
            "[step 500] loss=5.4650, ppl=236.28\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . reposait tarte . rosée O fleurs tranquilo dormia voler sala goûté Senti chaud claro pipas frescura dedos baignait confortável était\n",
            "--------------------------------------\n",
            "[step 550] loss=5.5661, ppl=261.41\n",
            "[step 600] loss=5.4054, ppl=222.61\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . blanc pluie rebord jouait tomber envolaient ervas ancienne sur la aromatiques plage moment tarte o fleur chant longe Les lune\n",
            "--------------------------------------\n",
            "[step 650] loss=5.3692, ppl=214.68\n",
            "[step 700] loss=5.1335, ppl=169.61\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . sofá cheia partir agradável lac manteiga pôr vidro lentamente réchauffait quentes levait un collines de . et iluminava assado rouges\n",
            "--------------------------------------\n",
            "[step 750] loss=5.3618, ppl=213.10\n",
            "[step 800] loss=5.2729, ppl=194.98\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . pátio odeur pôr parque trottoir le mãos aroma intérieure feuilles sorvete blancs da terre cuit minha passarem . reflexos fenêtre\n",
            "--------------------------------------\n",
            "[step 850] loss=5.1222, ppl=167.70\n",
            "[step 900] loss=5.0945, ppl=163.12\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . le estranhas melon apimentada pièce un revelava odeur verts couvertes faufiler chantait suculenta sofá O\n",
            "--------------------------------------\n",
            "[step 950] loss=5.0871, ppl=161.92\n",
            "[step 1000] loss=4.9069, ppl=135.21\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . J emplissait ma transformer senti tranquillement ranger . étroit lua brilhavam reflet briser toute épaules juteuse levait des encore légers\n",
            "--------------------------------------\n",
            "[step 1050] loss=4.9536, ppl=141.68\n",
            "[step 1100] loss=4.8537, ppl=128.21\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . frais acima pluie árvores lentement suavemente jardin . sala\n",
            "--------------------------------------\n",
            "[step 1150] loss=4.8960, ppl=133.75\n",
            "[step 1200] loss=4.8603, ppl=129.07\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . através eau blanc semblait paisible chien mon ’ fresco verts chão cantavam . rosée sur mãos loin paralelepípedos coussin .\n",
            "--------------------------------------\n",
            "[step 1250] loss=4.7582, ppl=116.53\n",
            "[step 1300] loss=4.7907, ppl=120.38\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . animait branco montagnes promener azul\n",
            "--------------------------------------\n",
            "[step 1350] loss=4.6164, ppl=101.13\n",
            "[step 1400] loss=4.5339, ppl=93.12\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . ondular parc\n",
            "--------------------------------------\n",
            "[step 1450] loss=4.6294, ppl=102.46\n",
            "[step 1500] loss=4.4898, ppl=89.11\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . frais luz confiture marshmallows le plume au moi acordavam colhida canela nature absoluta caía .\n",
            "--------------------------------------\n",
            "[step 1550] loss=4.3798, ppl=79.82\n",
            "[step 1600] loss=4.4523, ppl=85.83\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . . traversait bord leite delicioso jardim jardin . mains .\n",
            "--------------------------------------\n",
            "[step 1650] loss=4.5530, ppl=94.91\n",
            "[step 1700] loss=4.1988, ppl=66.61\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . pêra fort sur fort des pele vermelhas mar miel colina manhã humide coloridas esconde as le en du . ondas\n",
            "--------------------------------------\n",
            "[step 1750] loss=4.3236, ppl=75.46\n",
            "[step 1800] loss=4.3431, ppl=76.94\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . crocante enfants moído chá pluie dans air ciel caía briller pousavam riam florido pescoço papillon terra J maison fina\n",
            "--------------------------------------\n",
            "[step 1850] loss=4.4231, ppl=83.36\n",
            "[step 1900] loss=4.2239, ppl=68.30\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . paisible cour soleil bruit esgueirar matinale chêne en arc onduler fromage folhas changeaient soufflait feuilles chaleur Le vent . bol\n",
            "--------------------------------------\n",
            "[step 1950] loss=4.0407, ppl=56.87\n",
            "[step 2000] loss=4.1050, ppl=60.64\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . silence . mãos flaques froissait sobre poltrona forno .\n",
            "--------------------------------------\n",
            "[step 2050] loss=4.2594, ppl=70.77\n",
            "[step 2100] loss=3.9067, ppl=49.73\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . brise\n",
            "--------------------------------------\n",
            "[step 2150] loss=3.9886, ppl=53.98\n",
            "[step 2200] loss=4.0056, ppl=54.90\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . suavidade voar cidade paisible atrás transformarem frissonner . portait transeuntes transformer risadas aroma parecia noisette distante acima estremecer passants\n",
            "--------------------------------------\n",
            "[step 2250] loss=3.9368, ppl=51.25\n",
            "[step 2300] loss=4.2002, ppl=66.70\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . .\n",
            "--------------------------------------\n",
            "[step 2350] loss=3.5060, ppl=33.32\n",
            "[step 2400] loss=4.0750, ppl=58.85\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . .\n",
            "--------------------------------------\n",
            "[step 2450] loss=3.5858, ppl=36.08\n",
            "[step 2500] loss=3.9280, ppl=50.80\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie .\n",
            "--------------------------------------\n",
            "[step 2550] loss=3.5865, ppl=36.11\n",
            "[step 2600] loss=3.9979, ppl=54.49\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . chão forêt sucre pousavam thé brise parapeito était sol automne du animée formando . calma refléter na .\n",
            "--------------------------------------\n",
            "[step 2650] loss=3.9228, ppl=50.54\n",
            "[step 2700] loss=3.8723, ppl=48.05\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . chocolat du soir faufiler .\n",
            "--------------------------------------\n",
            "[step 2750] loss=3.6001, ppl=36.60\n",
            "[step 2800] loss=3.7254, ppl=41.49\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . rebord pain pão .\n",
            "--------------------------------------\n",
            "[step 2850] loss=3.4002, ppl=29.97\n",
            "[step 2900] loss=3.4352, ppl=31.04\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . fontaine sob alegremente sur mel odeur colorida .\n",
            "--------------------------------------\n",
            "[step 2950] loss=3.5195, ppl=33.77\n",
            "[step 3000] loss=3.2994, ppl=27.10\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . lentement fraîches .\n",
            "--------------------------------------\n",
            "[step 3050] loss=3.6882, ppl=39.97\n",
            "[step 3100] loss=3.4005, ppl=29.98\n",
            "Generated text:\n",
            "J ’ ai senti le parfum du pain chaud dans la boulangerie . cintilar esconde dans .\n",
            "--------------------------------------\n",
            "[step 3150] loss=3.6672, ppl=39.14\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-743749544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     loss = F.cross_entropy(\n\u001b[1;32m     32\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3404950455.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    629\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             x = self.norm1(\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m             )\n\u001b[1;32m   1125\u001b[0m             x = self.norm2(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mis_causal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     ) -> Tensor:\n\u001b[0;32m-> 1143\u001b[0;31m         x = self.self_attn(\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1486\u001b[0m             )\n\u001b[1;32m   1487\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1489\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6244\u001b[0m         \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6245\u001b[0m         \u001b[0mmask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"key_padding_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6246\u001b[0;31m         \u001b[0mother_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_none_or_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6247\u001b[0m         \u001b[0mother_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m         \u001b[0mtarget_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_none_or_dtype\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   6053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6055\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_none_or_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6056\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6057\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício: controle de temperatura, Top-K e Top-P\n",
        "\n",
        "Modifique o código a seguir para fazer a visualização de produção de tokens do modelo que você treinou."
      ],
      "metadata": {
        "id": "UHqvaoOWJ2yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# -----------------------------\n",
        "# Mock language-model logits\n",
        "# -----------------------------\n",
        "TOKENS = [\"blue\", \"purple\", \"violet\", \"vio\", \"not\", \"Blue\", \"green\", \"gray\", \"grey\", \"black\"]\n",
        "BASE_LOGITS = np.array([6.0, 1.5, 0.5, 0.2, 0.2, 0.0, -5.0, -5.0, -5.0, -5.0])\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))\n",
        "    return e / e.sum()\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    return logits / max(temperature, 1e-5)\n",
        "\n",
        "def apply_top_k(probs, k):\n",
        "    if k >= len(probs):\n",
        "        return probs\n",
        "    idx = np.argsort(probs)[::-1]\n",
        "    mask = np.zeros_like(probs)\n",
        "    mask[idx[:k]] = 1\n",
        "    probs = probs * mask\n",
        "    return probs / probs.sum()\n",
        "\n",
        "def apply_top_p(probs, p):\n",
        "    idx = np.argsort(probs)[::-1]\n",
        "    cumulative = np.cumsum(probs[idx])\n",
        "    mask = cumulative <= p\n",
        "    mask[np.argmax(mask)] = True\n",
        "    new_probs = np.zeros_like(probs)\n",
        "    new_probs[idx[mask]] = probs[idx[mask]]\n",
        "    return new_probs / new_probs.sum()\n",
        "\n",
        "# -----------------------------\n",
        "# Widgets\n",
        "# -----------------------------\n",
        "prompt_dropdown = widgets.Dropdown(\n",
        "    options=[\"Roses are red, violets are...\"],\n",
        "    value=\"Roses are red, violets are...\",\n",
        "    description=\"Prompt\",\n",
        "    layout=widgets.Layout(width=\"95%\")\n",
        ")\n",
        "\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=5.0, step=0.1,\n",
        "    description=\"Temperatura\",\n",
        "    readout_format=\".1f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=6, min=1, max=10, step=1,\n",
        "    description=\"Top-K\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topp_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=1.0, step=0.05,\n",
        "    description=\"Top-P\",\n",
        "    readout_format=\".2f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Plot\n",
        "# -----------------------------\n",
        "output_plot = widgets.Output()\n",
        "\n",
        "def update_plot(*args):\n",
        "    with output_plot:\n",
        "        output_plot.clear_output()\n",
        "\n",
        "        logits = apply_temperature(BASE_LOGITS, temperature_slider.value)\n",
        "        probs = softmax(logits)\n",
        "        probs = apply_top_k(probs, topk_slider.value)\n",
        "        probs = apply_top_p(probs, topp_slider.value)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        bars = ax.bar(TOKENS, probs * 100)\n",
        "        ax.set_ylim(0, 100)\n",
        "        ax.set_ylabel(\"Probabilidade (%)\")\n",
        "        ax.set_title(\"Probabilidade do próximo token\")\n",
        "\n",
        "        for bar, p in zip(bars, probs):\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_height(),\n",
        "                f\"{p*100:.2f}%\",\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=9\n",
        "            )\n",
        "\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.show()\n",
        "\n",
        "for w in [temperature_slider, topk_slider, topp_slider]:\n",
        "    w.observe(update_plot, names=\"value\")\n",
        "\n",
        "# -----------------------------\n",
        "# Collapsible explanation\n",
        "# -----------------------------\n",
        "accordion = widgets.Accordion(\n",
        "    children=[widgets.HTML(\n",
        "        \"\"\"\n",
        "        <ul>\n",
        "          <li><b>Temperatura</b>: Controla aleatoriedade com mudança na escala dos logits.</li>\n",
        "          <li><b>Top-K</b>: Restringe a amostra aos K tokens mais prováveis.</li>\n",
        "          <li><b>Top-P</b>: Usa somente os menor conjunto de tokens que resultam em probabilidade acumulada até P.</li>\n",
        "        </ul>\n",
        "        \"\"\"\n",
        "    )]\n",
        ")\n",
        "accordion.set_title(0, \"Entenda a visualização\")\n",
        "\n",
        "# -----------------------------\n",
        "# Layout\n",
        "# -----------------------------\n",
        "controls = widgets.VBox([\n",
        "    prompt_dropdown,\n",
        "    widgets.HBox([temperature_slider]),\n",
        "    widgets.HBox([topk_slider]),\n",
        "    widgets.HBox([topp_slider])\n",
        "])\n",
        "\n",
        "display(Markdown(\"## Visualização de controle de Temperatura, Top-K e Top-P.\"))\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<b>Parameters</b>\"),\n",
        "    controls,\n",
        "    output_plot,\n",
        "    accordion\n",
        "]))\n",
        "\n",
        "update_plot()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-3PHgy-JH6hM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}